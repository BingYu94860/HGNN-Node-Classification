{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:34:55.646496Z",
     "start_time": "2021-07-09T16:34:55.642507Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time, math\n",
    "\n",
    "# 選擇 圖資料集 # dataset_str => 'cora' or 'citeseer' or 'pubmed'\n",
    "dataset_str = 'cora'\n",
    "\n",
    "# P.S. pubmed資料集 還無法跑，尚有記憶體爆炸的bug\n",
    "\n",
    "# 包含自己的最近節點個數 # P.S. 論文中是不包含自己的最近 n 個\n",
    "n_neighbors = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:33:47.032630Z",
     "start_time": "2021-07-09T16:33:47.003614Z"
    }
   },
   "outputs": [],
   "source": [
    "def np_adj_to_dW(adj):\n",
    "    dW = sp.coo_matrix(adj)\n",
    "    dW = dW.toarray().astype('float32')\n",
    "    dW[dW == 0] = float('inf')\n",
    "    np.fill_diagonal(dW, 0)\n",
    "    return dW\n",
    "\n",
    "\n",
    "def tf_dist_W(dW, num_loop=None, batch_size=None, verbose=False):\n",
    "    num_nodes = dW.shape[0]\n",
    "    # 計算估計 max loop\n",
    "    max_loop = int(np.ceil(np.log2(num_nodes)))\n",
    "    if num_loop is not None:\n",
    "        max_loop = min(num_loop, max_loop)\n",
    "    # 設定 預設 batch_size\n",
    "    if batch_size is None:\n",
    "        if num_nodes >= 1000:\n",
    "            batch_size = 10\n",
    "        else:\n",
    "            batch_size = num_nodes\n",
    "    if verbose:\n",
    "        print(f'max_loop={max_loop}, batch_size={batch_size}')\n",
    "    # 迭代 max_loop 次\n",
    "    for i_loop in range(max_loop):\n",
    "        dW_odd = dW\n",
    "        A = tf.expand_dims(dW, 2)  # [N,N,1]\n",
    "        B = tf.expand_dims(dW, 0)  # [1,N,N]\n",
    "        # 以 batch 方式計算 dW = tf.reduce_min(A+B, 1)\n",
    "        ys = []\n",
    "        for ia in range(0, num_nodes, batch_size):\n",
    "            # 依序抽取 batch_size 個\n",
    "            ib = ia + batch_size\n",
    "            if ib >= num_nodes:\n",
    "                ib = num_nodes\n",
    "            iw = A[ia:ib]  # [batch_size,N,1]\n",
    "            # 計算 batch_size 個\n",
    "            ys.append(tf.reduce_min(iw + B, 1))\n",
    "        dW = tf.concat(ys, 0)\n",
    "        # 檢查如果 dW 和 dW_odd 則 提前停止\n",
    "        if tf.math.reduce_all(tf.equal(dW, dW_odd)):\n",
    "            break\n",
    "        if verbose:\n",
    "            print(f'run {i_loop}/{max_loop}')\n",
    "            print(f'{dW}')\n",
    "    return dW\n",
    "\n",
    "#==========#==========#==========#==========#==========#==========#==========#\n",
    "\n",
    "def tf_KNN_for_dW(dW, K=3):\n",
    "    num_nodes = dW.shape[0]\n",
    "    rr = tf.tile(tf.expand_dims(tf.range(num_nodes), -1), [1, K])\n",
    "    cc = tf.argsort(dW, -1, direction='ASCENDING')[:, :K]\n",
    "    indices = tf.stack([tf.reshape(rr, -1), tf.reshape(cc, -1)], -1)\n",
    "    indices = tf.cast(indices, 'int64')\n",
    "    values = tf.gather_nd(dW, indices)\n",
    "    y = tf.SparseTensor(indices, values, dense_shape=[num_nodes, num_nodes])\n",
    "    y = tf.sparse.to_dense(tf.sparse.reorder(y), default_value=math.inf)\n",
    "    return y  # [my-node, k-node]\n",
    "\n",
    "\n",
    "def get_H_run_KNeighbors(adj, n_neighbors=5, num_loop=None, batch_size=1, verbose=False):\n",
    "    dW = np_adj_to_dW(adj)  # [N,N]\n",
    "    dW = tf.convert_to_tensor(dW)\n",
    "    dW = tf_dist_W(dW, num_loop, batch_size, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(f'dW = \\n{dW}')\n",
    "    dW = tf_KNN_for_dW(dW, K=n_neighbors)\n",
    "    if verbose:\n",
    "        print(f'KNN={n_neighbors} => [my-node, k-node]\\n{dW}')\n",
    "    H = tf.linalg.matrix_transpose(dW)\n",
    "    H = tf.cast(tf.logical_not(tf.math.is_inf(H)), 'float32')\n",
    "    H = sp.coo_matrix(H.numpy())\n",
    "    if verbose:\n",
    "        print(f'H=\\n{H.toarray()}')\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T13:24:55.069602Z",
     "start_time": "2021-07-09T13:24:55.056638Z"
    }
   },
   "source": [
    "# 圖資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:35:10.200226Z",
     "start_time": "2021-07-09T16:34:59.698494Z"
    }
   },
   "outputs": [],
   "source": [
    "from planetoid_dataset import load_planetoid\n",
    "# 使用 planetoid_dataset.py 中的 load_planetoid 函數後，會自動去 下載planetoid內的檔案\n",
    "\n",
    "# https://github.com/kimiyoung/planetoid/tree/master/data\n",
    "# https://github.com/tkipf/gcn/tree/master/gcn/data\n",
    "# 以上兩個(planetoid和tkipf/gcn) 網址內都是檔案些是相同的，也是上面說的函數所下載的內容\n",
    "\n",
    "# 以 cora 資料集為例就會下載以下8個檔案至 ./download 內\n",
    "#　 'ind.cora.x', 'ind.cora.tx', 'ind.cora.allx',\n",
    "#　 'ind.cora.y', 'ind.cora.ty', 'ind.cora.ally',\n",
    "#　 'ind.cora.graph', 'ind.cora.test.index'\n",
    "\n",
    "gd = load_planetoid(dataset_str, folder='download', verbose=True)\n",
    "# dataset_str => 'cora' or 'citeseer' or 'pubmed'\n",
    "# folder => 為該目錄下的下載資料夾 ./download/\n",
    "\n",
    "print(f\"gd有以下keys：\\n{', '.join(list(gd.keys()))}\\n\")\n",
    "print(f\"共有{gd['num_nodes']}個節點, 且共分成{gd['num_classes']}類\")\n",
    "print(f\"{', '.join(gd['class_labels'])}\")\n",
    "\n",
    "# 得到鄰接矩陣\n",
    "adj = gd['adj']\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "\n",
    "# 使用 A+I 得到關聯矩陣 H\n",
    "H = get_H_run_KNeighbors(adj, n_neighbors=n_neighbors, verbose=True)\n",
    "\n",
    "# 計算 Dv^(-0.5)@H @ W@De^(-1) @ H@Dv^(-0.5)\n",
    "from utils.norm import norm_DvH_WDe_HDv\n",
    "from utils.convert import to_sparse_tensor\n",
    "\n",
    "DHWDHD = to_sparse_tensor(norm_DvH_WDe_HDv(H))\n",
    "\n",
    "supports = [DHWDHD]  # 預先計算好給 超圖卷積層 用的\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "\n",
    "# 得到節點特徵向量，並做正規化，再轉成稀疏張量\n",
    "# norm_DF => 做 D^(-1)@F , 也就是(正規化)讓每個 row 的合為 1\n",
    "from utils.norm import norm_DF\n",
    "features = to_sparse_tensor(norm_DF(gd['features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 得到 訓練/驗證/測試 的 標籤與遮罩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:38:09.944898Z",
     "start_time": "2021-07-09T16:38:09.927943Z"
    }
   },
   "outputs": [],
   "source": [
    "from planetoid_dataset import get_labels_mask\n",
    "\n",
    "# split => 'public' or 'random'\n",
    "# 'public' 為原本 \n",
    "gdx = get_labels_mask(dataset=gd,\n",
    "                      split='public',\n",
    "                      num_train_per_class=20,\n",
    "                      num_val=500,\n",
    "                      num_test=1000,\n",
    "                      verbose=True)\n",
    "\n",
    "print(f\"gdx有以下keys：\\n{', '.join(list(gdx.keys()))}\\n\")\n",
    "\n",
    "# 轉換成 張量\n",
    "train_label = tf.convert_to_tensor(gdx['y_train'], dtype='float32')\n",
    "val_label = tf.convert_to_tensor(gdx['y_val'], dtype='float32')\n",
    "test_label = tf.convert_to_tensor(gdx['y_test'], dtype='float32')\n",
    "train_mask = tf.convert_to_tensor(gdx['train_mask'])\n",
    "val_mask = tf.convert_to_tensor(gdx['val_mask'])\n",
    "test_mask = tf.convert_to_tensor(gdx['test_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 稀疏丟棄層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:38:11.069129Z",
     "start_time": "2021-07-09T16:38:11.058158Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def sparse_dropout(x, rate, noise_shape):\n",
    "    \"\"\"\n",
    "    Dropout for sparse tensors.\n",
    "    \"\"\"\n",
    "    random_tensor = 1 - rate\n",
    "    random_tensor += tf.random.uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse.retain(x, dropout_mask)\n",
    "    return pre_out * (1. / (1 - rate))\n",
    "class SparseDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, num_features_nonzero, name=None, **kwargs):\n",
    "        super(SparseDropout, self).__init__(name=name, **kwargs)\n",
    "        self.rate = rate\n",
    "        self.num_features_nonzero = num_features_nonzero\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        if training is not False:\n",
    "            x = sparse_dropout(x, self.rate, self.num_features_nonzero)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷積層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:38:12.087224Z",
     "start_time": "2021-07-09T16:38:12.070269Z"
    }
   },
   "outputs": [],
   "source": [
    "# supports 為一個 稀疏張量的 list\n",
    "# 如果是 第三代圖卷積，就會事先算好 D^(-0.5)@(A+In)@D^(-0.5) 後放進去\n",
    "# 如果是 超圖卷積，就會事先算好 Dv^(-0.5)@H @ W@De^(-1) @ H@Dv^(-0.5) 後放進去\n",
    "\n",
    "class GraphConvolution(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 supports,\n",
    "                 is_sparse_inputs=False,\n",
    "                 use_bias=True,\n",
    "                 activation=None,\n",
    "                 **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.supports = supports  # [sp_adj, ...]\n",
    "        self.num_K = len(supports)\n",
    "\n",
    "        self.is_sparse_inputs = is_sparse_inputs\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[-1]\n",
    "        self.output_dim = self.units\n",
    "        self.kernel = self.add_weight(\n",
    "            'kernel', [self.num_K, self.input_dim, self.output_dim],\n",
    "            initializer='random_normal')\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight('bias', [self.output_dim],\n",
    "                                        initializer='zeros')\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def dot(x, y, sparse=False):\n",
    "            if sparse:\n",
    "                res = tf.sparse.sparse_dense_matmul(x, y)\n",
    "            else:\n",
    "                res = tf.matmul(x, y)\n",
    "            return res\n",
    "\n",
    "        x = inputs\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(self.num_K):\n",
    "            pre_sup = dot(x, self.kernel[i], sparse=self.is_sparse_inputs)\n",
    "            support = dot(self.supports[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "        # bias\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "        # activation\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HGNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:38:13.007507Z",
     "start_time": "2021-07-09T16:38:12.997553Z"
    }
   },
   "outputs": [],
   "source": [
    "class HGNN(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero, supports,\n",
    "                 hidden1_dim, dropout, **kwargs):\n",
    "        super(HGNN, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.sp_dropout_layer = SparseDropout(dropout, num_features_nonzero)\n",
    "        self.gconv1 = GraphConvolution(units=hidden1_dim,\n",
    "                                       supports=supports,\n",
    "                                       is_sparse_inputs=True)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(dropout)\n",
    "        self.gconv2 = GraphConvolution(units=output_dim, supports=supports)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "\n",
    "        x = self.sp_dropout_layer(x, training)\n",
    "        x = self.gconv1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout_layer(x, training)\n",
    "        x = self.gconv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算具有遮罩 的 準確率 與 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:38:14.069580Z",
     "start_time": "2021-07-09T16:38:14.052625Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"\n",
    "    Softmax cross-entropy loss with masking.\n",
    "    \"\"\"\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"\n",
    "    Accuracy with masking.\n",
    "    \"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "def compute_loss_acc(output, label, mask, layer, weight_decay=5e-4):\n",
    "    # 計算 loss\n",
    "    loss = tf.zeros([])\n",
    "    for var in layer.trainable_variables:\n",
    "        loss += weight_decay * tf.nn.l2_loss(var)\n",
    "    loss += masked_softmax_cross_entropy(output, label, mask)\n",
    "    # 計算 acc\n",
    "    acc = masked_accuracy(output, label, mask)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T14:30:49.760120Z",
     "start_time": "2021-07-09T14:30:49.754137Z"
    }
   },
   "source": [
    "# 建立HGNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:38:15.175494Z",
     "start_time": "2021-07-09T16:38:15.135602Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "print(f\"num_features     = {gd['num_features']}\")\n",
    "print(f\"num_classes      = {gd['num_classes']}\")\n",
    "print(f\"num_features_nnz = {gd['num_features_nnz']}\") # 節點特徵矩陣 非0元素個數，給稀疏丟棄層使用\n",
    "\n",
    "model = HGNN(input_dim=gd['num_features'],\n",
    "            output_dim=gd['num_classes'],\n",
    "            supports=supports, # 預先計算 [DAD]\n",
    "            num_features_nonzero=(gd['num_features_nnz'], ),\n",
    "            hidden1_dim=16,\n",
    "            dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 運行模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:38:21.450190Z",
     "start_time": "2021-07-09T16:38:16.255830Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(model,\n",
    "              features,\n",
    "              train_label,\n",
    "              train_mask,\n",
    "              val_label,\n",
    "              val_mask,\n",
    "              test_label,\n",
    "              test_mask,\n",
    "              epochs=200,\n",
    "              lr=1e-2,\n",
    "              weight_decay=5e-4):\n",
    "    history_epoch = []\n",
    "    history_loss = []\n",
    "    history_accuracy = []\n",
    "    history_val_loss = []\n",
    "    history_val_accuracy = []\n",
    "    latents = []\n",
    "    #==========#==========#=====<>=====#==========#==========#\n",
    "    train_time = 0.0\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    for epoch in range(epochs):\n",
    "        #==========#==========#=====<train>=====#==========#==========#\n",
    "        st = time.time()\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = model(features)\n",
    "            loss, acc = compute_loss_acc(output, train_label, train_mask,\n",
    "                                         model.gconv1, weight_decay)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        #==========#==========#=====<val>=====#==========#==========#\n",
    "        output = model(features, training=False)\n",
    "        val_loss, val_acc = compute_loss_acc(output, val_label, val_mask,\n",
    "                                             model.gconv1, weight_decay)\n",
    "        epoch_time = time.time() - st\n",
    "        train_time += epoch_time\n",
    "        #==========#==========#=====<latent space>=====#==========#==========#\n",
    "        latent = model.gconv1(features).numpy()\n",
    "        latents.append(latent)\n",
    "        #==========#==========#=====<history>=====#==========#==========#\n",
    "        loss, acc = float(loss), float(acc)\n",
    "        val_loss, val_acc = float(val_loss), float(val_acc)\n",
    "        history_epoch.append(epoch)\n",
    "        history_loss.append(loss)\n",
    "        history_accuracy.append(acc)\n",
    "        history_val_loss.append(val_loss)\n",
    "        history_val_accuracy.append(val_acc)\n",
    "        if (epoch + 1) % 20 == 0 or epoch + 1 == epochs:\n",
    "            print(f\"{epoch+1:3}: loss={loss:.6f} acc={acc*100:10.6f}%\", end=\"\")\n",
    "            print(f\" | val_loss={val_loss:.6f} val_acc={val_acc*100:10.6f}%\")\n",
    "    #==========#==========#=====<Test>=====#==========#==========#\n",
    "    st = time.time()\n",
    "    output = model(features, training=False)\n",
    "    test_loss, test_acc = compute_loss_acc(output, test_label, test_mask,\n",
    "                                           model.gconv1, weight_decay)\n",
    "    test_time = time.time() - st\n",
    "    test_loss, test_acc = float(test_loss), float(test_acc)\n",
    "    print(f\"[Test]:\\t test_loss={test_loss:.6f} test_acc={test_acc*100:10.6f}%\")\n",
    "    #==========#==========#=====<>=====#==========#==========#\n",
    "    info = {}\n",
    "    info[\"train_acc\"] = acc\n",
    "    info[\"train_loss\"] = loss\n",
    "    info[\"val_acc\"] = val_acc\n",
    "    info[\"val_loss\"] = val_loss\n",
    "    info[\"test_acc\"] = test_acc\n",
    "    info[\"test_loss\"] = test_loss\n",
    "    info[\"train_time\"] = train_time\n",
    "    info[\"test_time\"] = test_time\n",
    "    info[\"total_params\"] = model.count_params()\n",
    "    info['epochs'] = epochs\n",
    "    info['optimizers'] = 'Adam'\n",
    "    info['learning_rate'] = lr\n",
    "    info['weight_decay'] = weight_decay\n",
    "    info[\"history_epoch\"] = history_epoch\n",
    "    info[\"history_loss\"] = history_loss\n",
    "    info[\"history_accuracy\"] = history_accuracy\n",
    "    info[\"history_val_loss\"] = history_val_loss\n",
    "    info[\"history_val_accuracy\"] = history_val_accuracy\n",
    "    return info, latents\n",
    "\n",
    "\n",
    "info, latents = run_model(model,\n",
    "                          features,\n",
    "                          train_label,\n",
    "                          train_mask,\n",
    "                          val_label,\n",
    "                          val_mask,\n",
    "                          test_label,\n",
    "                          test_mask,\n",
    "                          epochs=200,\n",
    "                          lr=1e-2,\n",
    "                          weight_decay=5e-4)\n",
    "\n",
    "# info 為訓練中紀錄的相關資訊\n",
    "print(f\"info有以下keys：\\n{', '.join(list(info.keys()))}\\n\")\n",
    "\n",
    "# latents 為每運行一次epoch後，紀錄 (不經稀疏丟棄層) 只經過第1個卷積後的輸出 #用於日後t-SNE畫圖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T15:02:58.009252Z",
     "start_time": "2021-07-09T15:02:57.996287Z"
    }
   },
   "source": [
    "# 準確率曲線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:38:22.575756Z",
     "start_time": "2021-07-09T16:38:21.968756Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_acc_curve(h_acc,\n",
    "                   h_val_acc,\n",
    "                   h_acc_label='Train Acc',\n",
    "                   h_val_acc_label='Val Acc',\n",
    "                   title='Accuracy curve of Model'):\n",
    "    fig, ax = plt.subplots(figsize=(8, 3), dpi=300)\n",
    "    ax.set_xlim(left=0, right=len(h_acc) * 1.01)\n",
    "    h_epoch = np.arange(len(h_acc)) + 1\n",
    "\n",
    "    ax.plot(h_epoch, h_acc, color='C0', label=h_acc_label, linestyle='-')\n",
    "    ax.plot(h_epoch, h_val_acc, color='C1', label=h_val_acc_label, linestyle='-')\n",
    "\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_ylabel('Accuracy', fontsize=10)\n",
    "    ax.set_xlabel('Epoch', fontsize=10)\n",
    "    ax.tick_params(pad=0.5, labelsize=8)\n",
    "    ax.legend(loc='best')\n",
    "    plt.close()\n",
    "    fig.savefig(fname=f'{title}.png',\n",
    "                bbox_inches='tight',\n",
    "                dpi=300,\n",
    "                pad_inches=0.0)\n",
    "    return fig\n",
    "\n",
    "\n",
    "show_acc_curve(h_acc=info['history_accuracy'], h_val_acc=info['history_val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T15:03:11.974709Z",
     "start_time": "2021-07-09T15:03:11.969724Z"
    }
   },
   "source": [
    "# loss曲線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:38:23.683116Z",
     "start_time": "2021-07-09T16:38:23.064510Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_loss_curve(h_loss,\n",
    "                    h_val_loss,\n",
    "                    h_loss_label='Train Loss',\n",
    "                    h_val_loss_label='Val Loss',\n",
    "                    title='Loss curve of Model'):\n",
    "    fig, ax = plt.subplots(figsize=(8, 3), dpi=300)\n",
    "    ax.set_xlim(left=0, right=len(h_loss) * 1.01)\n",
    "    h_epoch = np.arange(len(h_loss)) + 1\n",
    "\n",
    "    ax.plot(h_epoch, h_loss, color='C0', label=h_loss_label, linestyle='-')\n",
    "    ax.plot(h_epoch, h_val_loss, color='C1', label=h_val_loss_label, linestyle='-')\n",
    "\n",
    "    ax.set_title(title,\n",
    "                 fontsize=10)\n",
    "    ax.set_ylabel('Loss', fontsize=10)\n",
    "    ax.set_xlabel('Epoch', fontsize=10)\n",
    "    ax.tick_params(pad=0.5, labelsize=8)\n",
    "    ax.legend(loc='best')\n",
    "    plt.close()\n",
    "    fig.savefig(f'{title}.png', bbox_inches='tight', dpi=300, pad_inches=0.0)\n",
    "    return fig\n",
    "    \n",
    "show_loss_curve(h_loss=info['history_loss'], h_val_loss=info['history_val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 得到 t-SNE 將維後的座標 pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:38:36.372084Z",
     "start_time": "2021-07-09T16:38:24.203623Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.tsne import t_SNE, get_num_str_colors\n",
    "\n",
    "latent = latents[-1]  # 只看經最後一次epoch後的\n",
    "\n",
    "# 得到 經 t_SNE 降維 後的 2維座標\n",
    "pos = t_SNE(latent,\n",
    "            init='pca',\n",
    "            n_iter=1000,\n",
    "            verbose=1,\n",
    "            random_state=501,\n",
    "            is_norm=True,  # x 和 y 皆做正規化 0~1\n",
    "            is_pos=True)  # is_pos 轉成字典格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 畫圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T16:38:39.902130Z",
     "start_time": "2021-07-09T16:38:36.938998Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def show_graph(\n",
    "        adj,\n",
    "        pos,\n",
    "        class_names,  # class_labels\n",
    "        class_color,\n",
    "        w=10,\n",
    "        h=10,\n",
    "        with_labels=False,\n",
    "        alpha=1.0,\n",
    "        padding=0.5,\n",
    "        node_color='#8888FF',\n",
    "        node_size=1000,\n",
    "        node_shape='o',\n",
    "        edge_color='#000000',\n",
    "        edge_width=1.0,\n",
    "        edge_style='solid',\n",
    "        font_color='#000000',\n",
    "        font_size=12):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    G = nx.from_scipy_sparse_matrix(adj)\n",
    "    fig, ax = plt.subplots(figsize=(w, h))\n",
    "    nx.draw(G,\n",
    "            ax=ax,\n",
    "            pos=pos,\n",
    "            with_labels=with_labels,\n",
    "            node_color=node_color,\n",
    "            node_size=node_size,\n",
    "            node_shape=node_shape,\n",
    "            edge_color=edge_color,\n",
    "            width=edge_width,\n",
    "            style=edge_style,\n",
    "            font_color=font_color,\n",
    "            font_size=font_size)\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    ax.set_xlim(left=xmin - padding, right=xmax + padding)\n",
    "    ax.set_ylim(bottom=ymin - padding, top=ymax + padding)\n",
    "    if class_names is not None and class_color is not None:\n",
    "        for name, color in zip(class_names, class_color):\n",
    "            ax.scatter([], [], c=color, s=100, label=name)\n",
    "        ax.legend(frameon=False)\n",
    "    plt.close()\n",
    "    return fig\n",
    "\n",
    "adj = gd['adj']\n",
    "true_labels = gd['true_labels'] # 每個節點的類別編號\n",
    "class_labels = gd['class_labels'] # 類別標籤名稱\n",
    "\n",
    "# 針對 citeseer 沒有標籤(true_labels[i] == -1)的節點加上 'Null'\n",
    "if -1 in true_labels:\n",
    "    class_names = class_labels + ['Null']\n",
    "else:\n",
    "    class_names = class_labels\n",
    "\n",
    "# 各類別的色碼\n",
    "class_color = get_num_str_colors(len(class_names), True)\n",
    "# 轉換出 每一個節點對應顏色\n",
    "color_labels = class_color[true_labels]  # node_color\n",
    "\n",
    "fig = show_graph(\n",
    "    adj,\n",
    "    pos,\n",
    "    class_names=class_names,\n",
    "    class_color=class_color,\n",
    "    w=20,\n",
    "    h=20,\n",
    "    node_size=10,\n",
    "    edge_width=0.02,  #[0.1, 0.1, 0.02]\n",
    "    with_labels=False,\n",
    "    node_color=color_labels,\n",
    "    padding=0.0)\n",
    "\n",
    "fig.savefig(f\"graph.png\", bbox_inches='tight', dpi=300, pad_inches=0.0)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
