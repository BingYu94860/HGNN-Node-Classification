{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:11:23.464654Z",
     "start_time": "2021-07-30T02:11:21.214414Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import os, time\n",
    "\n",
    "# set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "from planetoid_dataset import load_planetoid, get_labels_mask\n",
    "from utils.norm import norm_DF, norm_DADsm #, norm_DvH_WDe_HDv\n",
    "from utils.convert import to_sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 稀疏丟棄層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:11:23.527133Z",
     "start_time": "2021-07-30T02:11:23.513171Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def sparse_dropout(x, rate, noise_shape):\n",
    "    \"\"\"\n",
    "    Dropout for sparse tensors.\n",
    "    \"\"\"\n",
    "    random_tensor = 1 - rate\n",
    "    random_tensor += tf.random.uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse.retain(x, dropout_mask)\n",
    "    return pre_out * (1. / (1 - rate))\n",
    "\n",
    "\n",
    "class SparseDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, num_features_nonzero, name=None, **kwargs):\n",
    "        super(SparseDropout, self).__init__(name=name, **kwargs)\n",
    "        self.rate = rate\n",
    "        self.num_features_nonzero = num_features_nonzero\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        if training is not False:\n",
    "            x = sparse_dropout(x, self.rate, self.num_features_nonzero)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷積層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:11:23.590014Z",
     "start_time": "2021-07-30T02:11:23.576052Z"
    }
   },
   "outputs": [],
   "source": [
    "# supports 為一個 稀疏張量的 list\n",
    "# 如果是 第三代圖卷積，就會事先算好 D^(-0.5)@(A+In)@D^(-0.5) 後放進去\n",
    "# 如果是 超圖卷積，就會事先算好 Dv^(-0.5)@H @ W@De^(-1) @ H@Dv^(-0.5) 後放進去\n",
    "\n",
    "class GraphConvolution(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 supports,\n",
    "                 is_sparse_inputs=False,\n",
    "                 use_bias=True,\n",
    "                 activation=None,\n",
    "                 **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.supports = supports  # [sp_adj, ...]\n",
    "        self.num_K = len(supports)\n",
    "\n",
    "        self.is_sparse_inputs = is_sparse_inputs\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[-1]\n",
    "        self.output_dim = self.units\n",
    "        self.kernel = self.add_weight(\n",
    "            'kernel', [self.num_K, self.input_dim, self.output_dim],\n",
    "            initializer='random_normal')\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight('bias', [self.output_dim],\n",
    "                                        initializer='zeros')\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def dot(x, y, sparse=False):\n",
    "            if sparse:\n",
    "                res = tf.sparse.sparse_dense_matmul(x, y)\n",
    "            else:\n",
    "                res = tf.matmul(x, y)\n",
    "            return res\n",
    "\n",
    "        x = inputs\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(self.num_K):\n",
    "            pre_sup = dot(x, self.kernel[i], sparse=self.is_sparse_inputs)\n",
    "            support = dot(self.supports[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "        # bias\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "        # activation\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:11:24.385781Z",
     "start_time": "2021-07-30T02:11:24.374810Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCN(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero, supports,\n",
    "                 hidden1_dim, dropout, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.sp_dropout_layer = SparseDropout(dropout, num_features_nonzero)\n",
    "        self.gconv1 = GraphConvolution(units=hidden1_dim,\n",
    "                                       supports=supports,\n",
    "                                       is_sparse_inputs=True)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(dropout)\n",
    "        self.gconv2 = GraphConvolution(units=output_dim, supports=supports)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "\n",
    "        x = self.sp_dropout_layer(x, training)\n",
    "        x = self.gconv1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout_layer(x, training)\n",
    "        x = self.gconv2(x)\n",
    "        return x\n",
    "    \n",
    "class GCN_v2(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero, supports,\n",
    "                 hidden1_dim, dropout, **kwargs):\n",
    "        super(GCN_v2, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        ## self.sp_dropout_layer = SparseDropout(dropout, num_features_nonzero)\n",
    "        self.gconv1 = GraphConvolution(units=hidden1_dim,\n",
    "                                       supports=supports,\n",
    "                                       is_sparse_inputs=True)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "\n",
    "        ## self.dropout_layer = tf.keras.layers.Dropout(dropout)\n",
    "        self.gconv2 = GraphConvolution(units=output_dim, supports=supports)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "\n",
    "        ## x = self.sp_dropout_layer(x, training)\n",
    "        x = self.gconv1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        ## x = self.dropout_layer(x, training)\n",
    "        x = self.gconv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算具有遮罩 的 準確率 與 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:11:25.463578Z",
     "start_time": "2021-07-30T02:11:25.446624Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"\n",
    "    Softmax cross-entropy loss with masking.\n",
    "    \"\"\"\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"\n",
    "    Accuracy with masking.\n",
    "    \"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "def compute_loss_acc(output, label, mask, layer, weight_decay=5e-4):\n",
    "    # 計算 loss\n",
    "    loss = tf.zeros([])\n",
    "    for var in layer.trainable_variables:\n",
    "        loss += weight_decay * tf.nn.l2_loss(var)\n",
    "    loss += masked_softmax_cross_entropy(output, label, mask)\n",
    "    # 計算 acc\n",
    "    acc = masked_accuracy(output, label, mask)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T07:44:30.125558Z",
     "start_time": "2021-07-12T07:44:30.121654Z"
    }
   },
   "source": [
    "# def  Run Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:11:26.435042Z",
     "start_time": "2021-07-30T02:11:26.419085Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(model,\n",
    "              features,\n",
    "              train_label,\n",
    "              train_mask,\n",
    "              val_label,\n",
    "              val_mask,\n",
    "              test_label,\n",
    "              test_mask,\n",
    "              epochs=200,\n",
    "              lr=1e-2,\n",
    "              weight_decay=5e-4):\n",
    "    history_epoch = []\n",
    "    history_loss = []\n",
    "    history_accuracy = []\n",
    "    history_val_loss = []\n",
    "    history_val_accuracy = []\n",
    "    latents = []\n",
    "    #==========#==========#=====<>=====#==========#==========#\n",
    "    train_time = 0.0\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    for epoch in range(epochs):\n",
    "        #==========#==========#=====<train>=====#==========#==========#\n",
    "        st = time.time()\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = model(features)\n",
    "            loss, acc = compute_loss_acc(output, train_label, train_mask,\n",
    "                                         model.gconv1, weight_decay)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        #==========#==========#=====<val>=====#==========#==========#\n",
    "        output = model(features, training=False)\n",
    "        val_loss, val_acc = compute_loss_acc(output, val_label, val_mask,\n",
    "                                             model.gconv1, weight_decay)\n",
    "        epoch_time = time.time() - st\n",
    "        train_time += epoch_time\n",
    "        #==========#==========#=====<latent space>=====#==========#==========#\n",
    "        latent = model.gconv1(features).numpy()\n",
    "        latents.append(latent)\n",
    "        #==========#==========#=====<history>=====#==========#==========#\n",
    "        loss, acc = float(loss), float(acc)\n",
    "        val_loss, val_acc = float(val_loss), float(val_acc)\n",
    "        history_epoch.append(epoch)\n",
    "        history_loss.append(loss)\n",
    "        history_accuracy.append(acc)\n",
    "        history_val_loss.append(val_loss)\n",
    "        history_val_accuracy.append(val_acc)\n",
    "        if (epoch + 1) % 20 == 0 or epoch + 1 == epochs:\n",
    "            print(f\"{epoch+1:3}: loss={loss:.6f} acc={acc*100:10.6f}%\", end=\"\")\n",
    "            print(f\" | val_loss={val_loss:.6f} val_acc={val_acc*100:10.6f}%\")\n",
    "    #==========#==========#=====<Test>=====#==========#==========#\n",
    "    st = time.time()\n",
    "    output = model(features, training=False)\n",
    "    test_loss, test_acc = compute_loss_acc(output, test_label, test_mask,\n",
    "                                           model.gconv1, weight_decay)\n",
    "    test_time = time.time() - st\n",
    "    test_loss, test_acc = float(test_loss), float(test_acc)\n",
    "    print(f\"[Test]:\\t test_loss={test_loss:.6f} test_acc={test_acc*100:10.6f}%\")\n",
    "    #==========#==========#=====<>=====#==========#==========#\n",
    "    info = {}\n",
    "    info[\"train_acc\"] = acc\n",
    "    info[\"train_loss\"] = loss\n",
    "    info[\"val_acc\"] = val_acc\n",
    "    info[\"val_loss\"] = val_loss\n",
    "    info[\"test_acc\"] = test_acc\n",
    "    info[\"test_loss\"] = test_loss\n",
    "    info[\"train_time\"] = train_time\n",
    "    info[\"test_time\"] = test_time\n",
    "    info[\"total_params\"] = model.count_params()\n",
    "    info['epochs'] = epochs\n",
    "    info['optimizers'] = 'Adam'\n",
    "    info['learning_rate'] = lr\n",
    "    info['weight_decay'] = weight_decay\n",
    "    info[\"history_epoch\"] = history_epoch\n",
    "    info[\"history_loss\"] = history_loss\n",
    "    info[\"history_accuracy\"] = history_accuracy\n",
    "    info[\"history_val_loss\"] = history_val_loss\n",
    "    info[\"history_val_accuracy\"] = history_val_accuracy\n",
    "    return info, latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:11:26.964737Z",
     "start_time": "2021-07-30T02:11:26.946786Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_json(info, folder='output', file_name='info', verbose=True):\n",
    "    info['save_time'] = get_now_time_str()\n",
    "    info['GPU'] = get_nvidia_gpu_name()\n",
    "    \n",
    "    # 建立 儲存輸出 的資料夾\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "        print(f'mkdir: ./{folder}')\n",
    "\n",
    "    # save info.json\n",
    "    file_path = os.path.join(folder, f\"{file_name}~{info['save_time']}.json\")\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(info, f, indent=4)\n",
    "\n",
    "    if verbose: print(f\"Save {file_path}\")\n",
    "\n",
    "\n",
    "def get_now_time_str() -> str:\n",
    "    import datetime\n",
    "    return (datetime.datetime.utcnow() +\n",
    "            datetime.timedelta(hours=8)).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def get_nvidia_gpu_name(verbose=False) -> str:\n",
    "    import subprocess\n",
    "    sP = subprocess.Popen(['nvidia-smi', '-L'],\n",
    "                          stdout=subprocess.PIPE,\n",
    "                          stderr=subprocess.PIPE)\n",
    "    out_str = sP.communicate()[0].decode()\n",
    "    first_line = out_str.split('\\n')[0]\n",
    "    if verbose:\n",
    "        print(first_line)\n",
    "    if 'failed' in first_line.split():\n",
    "        return 'No GPU'\n",
    "    gpu_name = first_line[7:first_line.find(' (UUID:')]\n",
    "    return gpu_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:11:28.041925Z",
     "start_time": "2021-07-30T02:11:28.022976Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_all(dataset_str='cora', model_str='GCN'):\n",
    "    # dataset_str => 'cora' or 'citeseer' or 'pubmed'\n",
    "\n",
    "    gd = load_planetoid(dataset_str, folder='download', verbose=False)\n",
    "    gdx = get_labels_mask(dataset=gd,\n",
    "                          split='random',\n",
    "                          num_train_per_class=20,\n",
    "                          num_val=500,\n",
    "                          num_test=1000,\n",
    "                          verbose=False)\n",
    "\n",
    "    adj = gd['adj']\n",
    "\n",
    "    if model_str in ['GCN', 'GCN_v2']:\n",
    "        DAD = to_sparse_tensor(norm_DADsm(adj))\n",
    "        supports = [DAD]  # 預先計算好給第三代圖卷積層用的\n",
    "    else:\n",
    "        assert True, '未定義模型'\n",
    "\n",
    "    features = to_sparse_tensor(norm_DF(gd['features']))\n",
    "\n",
    "    train_label = tf.convert_to_tensor(gdx['y_train'], dtype='float32')\n",
    "    val_label = tf.convert_to_tensor(gdx['y_val'], dtype='float32')\n",
    "    test_label = tf.convert_to_tensor(gdx['y_test'], dtype='float32')\n",
    "    train_mask = tf.convert_to_tensor(gdx['train_mask'])\n",
    "    val_mask = tf.convert_to_tensor(gdx['val_mask'])\n",
    "    test_mask = tf.convert_to_tensor(gdx['test_mask'])\n",
    "    \n",
    "    if model_str == 'GCN':\n",
    "        model = GCN(\n",
    "            input_dim=gd['num_features'],\n",
    "            output_dim=gd['num_classes'],\n",
    "            supports=supports,  # 預先計算 [DAD]\n",
    "            num_features_nonzero=(gd['num_features_nnz'], ),\n",
    "            hidden1_dim=16,\n",
    "            dropout=0.5)\n",
    "    elif model_str == 'GCN_v2':\n",
    "        model = GCN_v2(\n",
    "            input_dim=gd['num_features'],\n",
    "            output_dim=gd['num_classes'],\n",
    "            supports=supports,  # 預先計算 [DAD]\n",
    "            num_features_nonzero=(gd['num_features_nnz'], ),\n",
    "            hidden1_dim=16,\n",
    "            dropout=0.5)\n",
    "        \n",
    "    info, latents = run_model(model,\n",
    "                              features,\n",
    "                              train_label,\n",
    "                              train_mask,\n",
    "                              val_label,\n",
    "                              val_mask,\n",
    "                              test_label,\n",
    "                              test_mask,\n",
    "                              epochs=200,\n",
    "                              lr=1e-2,\n",
    "                              weight_decay=5e-4)\n",
    "\n",
    "    info['dataset'] = dataset_str\n",
    "    info['model'] = model_str\n",
    "    \n",
    "    save_json(info,\n",
    "              folder='output',\n",
    "              file_name=f\"{info['model']}_{info['dataset']}_info\",\n",
    "              verbose=True)\n",
    "    \n",
    "    return info, latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:12:10.354623Z",
     "start_time": "2021-07-30T02:12:06.511703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20: loss=1.874382 acc= 68.571424% | val_loss=1.912477 val_acc= 37.999997%\n",
      " 40: loss=1.625763 acc= 90.714282% | val_loss=1.798631 val_acc= 63.599992%\n",
      " 60: loss=1.291670 acc= 95.714289% | val_loss=1.620469 val_acc= 73.199999%\n",
      " 80: loss=0.974447 acc= 98.571426% | val_loss=1.444352 val_acc= 76.000005%\n",
      "100: loss=0.850143 acc= 97.857136% | val_loss=1.316899 val_acc= 75.799996%\n",
      "120: loss=0.698254 acc= 98.571438% | val_loss=1.230059 val_acc= 75.599992%\n",
      "140: loss=0.676075 acc= 97.857136% | val_loss=1.149409 val_acc= 76.399994%\n",
      "160: loss=0.594039 acc= 99.285716% | val_loss=1.137829 val_acc= 77.399999%\n",
      "180: loss=0.550133 acc= 98.571438% | val_loss=1.075250 val_acc= 77.999991%\n",
      "200: loss=0.545751 acc= 97.857136% | val_loss=1.068479 val_acc= 77.199996%\n",
      "[Test]:\t test_loss=1.040813 test_acc= 78.799993%\n",
      "Save output\\GCN_cora_info~20210730_101210.json\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    info, latents = run_all(dataset_str='cora', model_str='GCN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    info, latents = run_all(dataset_str='citeseer', model_str='GCN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    info, latents = run_all(dataset_str='pubmed', model_str='GCN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:12:35.448951Z",
     "start_time": "2021-07-30T02:12:33.139125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20: loss=1.849185 acc= 76.428562% | val_loss=1.898966 val_acc= 54.399997%\n",
      " 40: loss=1.532749 acc= 92.857140% | val_loss=1.771338 val_acc= 74.599993%\n",
      " 60: loss=1.168311 acc= 95.714277% | val_loss=1.596813 val_acc= 75.199997%\n",
      " 80: loss=0.914476 acc= 97.142869% | val_loss=1.434797 val_acc= 75.799996%\n",
      "100: loss=0.760971 acc= 97.857153% | val_loss=1.321933 val_acc= 76.399994%\n",
      "120: loss=0.663273 acc= 98.571438% | val_loss=1.243966 val_acc= 77.799988%\n",
      "140: loss=0.595355 acc= 98.571438% | val_loss=1.188720 val_acc= 77.999991%\n",
      "160: loss=0.544738 acc= 98.571438% | val_loss=1.146827 val_acc= 78.599989%\n",
      "180: loss=0.505080 acc= 99.285716% | val_loss=1.113935 val_acc= 78.799993%\n",
      "200: loss=0.472822 acc= 99.285716% | val_loss=1.086301 val_acc= 78.599989%\n",
      "[Test]:\t test_loss=1.089009 test_acc= 78.299999%\n",
      "Save output\\GCN_v2_cora_info~20210730_101235.json\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    info, latents = run_all(dataset_str='cora', model_str='GCN_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    info, latents = run_all(dataset_str='citeseer', model_str='GCN_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    info, latents = run_all(dataset_str='pubmed', model_str='GCN_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T15:02:58.009252Z",
     "start_time": "2021-07-09T15:02:57.996287Z"
    }
   },
   "source": [
    "# 準確率曲線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T06:42:34.446414Z",
     "start_time": "2021-07-12T06:42:33.624712Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def show_acc_curve(h_acc,\n",
    "                   h_val_acc,\n",
    "                   h_acc_label='Train Acc',\n",
    "                   h_val_acc_label='Val Acc',\n",
    "                   title='Accuracy curve of Model'):\n",
    "    fig, ax = plt.subplots(figsize=(8, 3), dpi=300)\n",
    "    ax.set_xlim(left=0, right=len(h_acc) * 1.01)\n",
    "    h_epoch = np.arange(len(h_acc)) + 1\n",
    "\n",
    "    ax.plot(h_epoch, h_acc, color='C0', label=h_acc_label, linestyle='-')\n",
    "    ax.plot(h_epoch, h_val_acc, color='C1', label=h_val_acc_label, linestyle='-')\n",
    "\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_ylabel('Accuracy', fontsize=10)\n",
    "    ax.set_xlabel('Epoch', fontsize=10)\n",
    "    ax.tick_params(pad=0.5, labelsize=8)\n",
    "    ax.legend(loc='best')\n",
    "    plt.close()\n",
    "    #fig.savefig(fname=f'{title}.png', bbox_inches='tight', dpi=300, pad_inches=0.0)\n",
    "    return fig\n",
    "\n",
    "\n",
    "show_acc_curve(h_acc=info['history_accuracy'], h_val_acc=info['history_val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T15:03:11.974709Z",
     "start_time": "2021-07-09T15:03:11.969724Z"
    }
   },
   "source": [
    "# loss曲線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T06:42:35.443336Z",
     "start_time": "2021-07-12T06:42:34.636741Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def show_loss_curve(h_loss,\n",
    "                    h_val_loss,\n",
    "                    h_loss_label='Train Loss',\n",
    "                    h_val_loss_label='Val Loss',\n",
    "                    title='Loss curve of Model'):\n",
    "    fig, ax = plt.subplots(figsize=(8, 3), dpi=300)\n",
    "    ax.set_xlim(left=0, right=len(h_loss) * 1.01)\n",
    "    h_epoch = np.arange(len(h_loss)) + 1\n",
    "\n",
    "    ax.plot(h_epoch, h_loss, color='C0', label=h_loss_label, linestyle='-')\n",
    "    ax.plot(h_epoch, h_val_loss, color='C1', label=h_val_loss_label, linestyle='-')\n",
    "\n",
    "    ax.set_title(title,\n",
    "                 fontsize=10)\n",
    "    ax.set_ylabel('Loss', fontsize=10)\n",
    "    ax.set_xlabel('Epoch', fontsize=10)\n",
    "    ax.tick_params(pad=0.5, labelsize=8)\n",
    "    ax.legend(loc='best')\n",
    "    plt.close()\n",
    "    #fig.savefig(f'{title}.png', bbox_inches='tight', dpi=300, pad_inches=0.0)\n",
    "    return fig\n",
    "    \n",
    "show_loss_curve(h_loss=info['history_loss'], h_val_loss=info['history_val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 得到 t-SNE 將維後的座標 pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T15:40:46.043721Z",
     "start_time": "2021-07-09T15:40:33.422529Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from utils.tsne import t_SNE, get_num_str_colors\n",
    "\n",
    "latent = latents[-1]  # 只看經最後一次epoch後的\n",
    "\n",
    "# 得到 經 t_SNE 降維 後的 2維座標\n",
    "pos = t_SNE(latent,\n",
    "            init='pca',\n",
    "            n_iter=1000,\n",
    "            verbose=1,\n",
    "            random_state=501,\n",
    "            is_norm=True,  # x 和 y 皆做正規化 0~1\n",
    "            is_pos=True)  # is_pos 轉成字典格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 畫圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T15:40:49.157666Z",
     "start_time": "2021-07-09T15:40:46.045716Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def show_graph(\n",
    "        adj,\n",
    "        pos,\n",
    "        class_names,  # class_labels\n",
    "        class_color,\n",
    "        w=10,\n",
    "        h=10,\n",
    "        with_labels=False,\n",
    "        alpha=1.0,\n",
    "        padding=0.5,\n",
    "        node_color='#8888FF',\n",
    "        node_size=1000,\n",
    "        node_shape='o',\n",
    "        edge_color='#000000',\n",
    "        edge_width=1.0,\n",
    "        edge_style='solid',\n",
    "        font_color='#000000',\n",
    "        font_size=12):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    G = nx.from_scipy_sparse_matrix(adj)\n",
    "    fig, ax = plt.subplots(figsize=(w, h))\n",
    "    nx.draw(G,\n",
    "            ax=ax,\n",
    "            pos=pos,\n",
    "            with_labels=with_labels,\n",
    "            node_color=node_color,\n",
    "            node_size=node_size,\n",
    "            node_shape=node_shape,\n",
    "            edge_color=edge_color,\n",
    "            width=edge_width,\n",
    "            style=edge_style,\n",
    "            font_color=font_color,\n",
    "            font_size=font_size)\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    ax.set_xlim(left=xmin - padding, right=xmax + padding)\n",
    "    ax.set_ylim(bottom=ymin - padding, top=ymax + padding)\n",
    "    if class_names is not None and class_color is not None:\n",
    "        for name, color in zip(class_names, class_color):\n",
    "            ax.scatter([], [], c=color, s=100, label=name)\n",
    "        ax.legend(frameon=False)\n",
    "    plt.close()\n",
    "    return fig\n",
    "\n",
    "adj = gd['adj']\n",
    "true_labels = gd['true_labels'] # 每個節點的類別編號\n",
    "class_labels = gd['class_labels'] # 類別標籤名稱\n",
    "\n",
    "# 針對 citeseer 沒有標籤(true_labels[i] == -1)的節點加上 'Null'\n",
    "if -1 in true_labels:\n",
    "    class_names = class_labels + ['Null']\n",
    "else:\n",
    "    class_names = class_labels\n",
    "\n",
    "# 各類別的色碼\n",
    "class_color = get_num_str_colors(len(class_names), True)\n",
    "# 轉換出 每一個節點對應顏色\n",
    "color_labels = class_color[true_labels]  # node_color\n",
    "\n",
    "fig = show_graph(\n",
    "    adj,\n",
    "    pos,\n",
    "    class_names=class_names,\n",
    "    class_color=class_color,\n",
    "    w=20,\n",
    "    h=20,\n",
    "    node_size=10,\n",
    "    edge_width=0.02,  #[0.1, 0.1, 0.02]\n",
    "    with_labels=False,\n",
    "    node_color=color_labels,\n",
    "    padding=0.0)\n",
    "\n",
    "fig.savefig(f\"graph.png\", bbox_inches='tight', dpi=300, pad_inches=0.0)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
